{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "pleasant-layout",
   "metadata": {},
   "source": [
    "# Loss Function: \n",
    "\n",
    "La Loss Function o Cost Function es la funcion que elijamos para calcular cómo está performando nuestro modelo. Esta función mapea los valores de las variables del modelo con un número real que representa el \"costo\" que esa version del modelo tiene respecto a los valores desables. Idealmente, el objetivo del modelo es optimizar el valor de los weights and biases hasta obtener el costo más bajo, para ello. \n",
    "\n",
    "Intuitivamente diría que esta función se utiliza con modelos supervisados porque entre sus parametros recibe los targets o labels del modelo, pero esto habría que revisarlo mejor.\n",
    "\n",
    "En la bibliografía e incluso en pytorch, esta función recibe distintos nombre:\n",
    "\n",
    "- error function\n",
    "- criterion function\n",
    "- cost function\n",
    "- objective function\n",
    "- loss function\n",
    "\n",
    "Si bien, en casos particulares todas la denominaciones hacen referencia a lo mismo, esto no es estrictamente así visto en detalle.\n",
    "\n",
    "Una diferencia entre funciones cost, loss y objective esta en este [comentario de stack exchange](https://stats.stackexchange.com/questions/179026/objective-function-cost-function-loss-function-are-they-the-same-thing)\n",
    "\n",
    "La noción de 'criterion' y la idea general de función loss que viene del decision theory se me escapa bastante.\n",
    "\n",
    "\n",
    "-------------------------------------------------\n",
    "<sub>To do: explicar estas diferencias de denominación</sub>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "intense-extreme",
   "metadata": {},
   "source": [
    "## Por qué funciona?\n",
    "\n",
    "Ok, pero cómo saber qué cambios debemos hacer en nuestros w&b que se inicializaron con valores random, para obtener el resultado más bajo de nuestra función loss? \n",
    "\n",
    "Dejemos que alguien lo explique mejor, como, por ejemplo, este [post de kaggle](https://www.kaggle.com/erdemuysal/linear-regression-from-scratch-and-with-pytorch)\n",
    "\n",
    "\"The loss is a quadratic function of our weights and biases, and our objective is to find the set of weights where the loss is the lowest. If we plot a graph of the loss w.r.t any individual weight or bias element, it will look like the figure shown below. A key insight from calculus is that the gradient indicates the rate of change of the loss, or the slope of the loss function w.r.t. the weights and biases.\n",
    "\n",
    "<img src=\"LOSSIMAGE.png\">\n",
    "\n",
    "\n",
    "The increase or decrease in loss by changing a weight element is proportional to the value of the gradient of the loss w.r.t. that element. This forms the basis for the optimization algorithm that we'll use to improve our model.\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "optical-chrome",
   "metadata": {},
   "source": [
    "# Distintas Loss Functions y su aplicación\n",
    "\n",
    "## Problemas de regresión \n",
    "\n",
    "Para problemas de regresión[<sup>1</sup>](#fn1), una de las funciones loss más usadas es el Mean Squared Error, pero también pueden aplicarse  Mean Absolute Error u otras y su elección depende de la distribución de los datos, la presencia de outliers, factores que habría que analizar al momento de tener un corpus.\n",
    "\n",
    "\n",
    "### Mean Squared Error Loss\n",
    "\n",
    "Esta función toma los valores esperados (y) y los valores de la prediccion (y_hat) y suma el total de sus distancias cuadradas. El total de esta suma es promediado por el numero de ejemplos.\n",
    "\n",
    "\n",
    "<img src=\"MSEL.png\">\n",
    "\n",
    "----------------------------------------------------------------------------------------------------------------\n",
    "<sub>[<sup id=\"fn1\">1</sup>](#fn1-back)Un problema de regresión es aquel cuyas variables de salida (o outputs) son valores reales o continuos, como 'salario', 'peso', 'costo de una propiedad'.</span></sub>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "descending-techno",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output:  tensor([[ 0.4886,  0.0048, -1.2792, -0.8813,  1.5077],\n",
      "        [ 0.4006,  0.4693, -1.1326, -0.7136,  1.4769],\n",
      "        [ 0.4852, -0.1870, -1.1527,  0.2586, -0.6437]], requires_grad=True)\n",
      "targets:  tensor([[ 1.1903,  0.7374,  0.3875,  0.2201,  0.0893],\n",
      "        [ 1.1994,  0.0387, -0.5466,  0.6031,  1.2106],\n",
      "        [ 0.3112, -0.1751,  0.6916,  1.9733, -0.2962]])\n",
      "-------------------\n"
     ]
    }
   ],
   "source": [
    "#Implementación en Pytorch\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "mse_loss = nn.MSELoss()\n",
    "outputs = torch.randn(3, 5, requires_grad=True)\n",
    "targets = torch.randn(3, 5)\n",
    "loss = mse_loss(outputs, targets)\n",
    "print('output: ', outputs)\n",
    "print('targets: ', targets)\n",
    "print('-------------------')\n",
    "#print('loss: ', loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "rental-processor",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sum_se tensor([0., 0., 0., 0., 0.])\n",
      "se tensor([0.4924, 0.5367, 2.7781, 1.2131, 2.0119], grad_fn=<PowBackward0>)\n",
      "sum_se tensor([0.4924, 0.5367, 2.7781, 1.2131, 2.0119], grad_fn=<AddBackward0>)\n",
      "se tensor([0.6380, 0.1854, 0.3433, 1.7337, 0.0709], grad_fn=<PowBackward0>)\n",
      "sum_se tensor([1.1304, 0.7221, 3.1214, 2.9468, 2.0828], grad_fn=<AddBackward0>)\n",
      "se tensor([3.0256e-02, 1.4266e-04, 3.4015e+00, 2.9402e+00, 1.2071e-01],\n",
      "       grad_fn=<PowBackward0>)\n",
      "sum_se tensor([1.1607, 0.7223, 6.5229, 5.8870, 2.2035], grad_fn=<AddBackward0>)\n",
      "end tensor([1.1607, 0.7223, 6.5229, 5.8870, 2.2035], grad_fn=<AddBackward0>)\n",
      "tensor(16.4964, grad_fn=<SumBackward0>)\n",
      "tensor(1.0998, grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Cuenta propia para ver cómo funciona\n",
    "\n",
    "sum_se = torch.zeros(5)\n",
    "print('sum_se', sum_se)\n",
    "for i in zip(targets, outputs):\n",
    "    se = (i[0] - i[1]) ** 2\n",
    "    print('se', se)\n",
    "    sum_se += se\n",
    "    print('sum_se', sum_se)\n",
    "print('end', sum_se)\n",
    "    \n",
    "sum_num = torch.sum(sum_se)\n",
    "print(sum_num)\n",
    "\n",
    "mse = sum_num / 15\n",
    "#mse = sum_num / len(targets) descomentar para ver el resultado de interpretar N como len de targets en vez de MxN\n",
    "print(mse)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "charged-sterling",
   "metadata": {},
   "source": [
    "### Por qué 15?\n",
    "\n",
    "Cuando tomé la primera descripción de la función 'no me daban los números' y mi resultado no era el mismo que el de aplicar la función de pytorch, hasta que encontré esta descripción que hace explicito que los valores involucrados no son puntos sino matrices mxn.\n",
    "\n",
    "<img src=\"MSELMxN.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "auburn-invention",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Linear Regression with california housing dataset (default available in colab)\n",
    "\n",
    "Con este ejemplo quise ilustrar de manera simple el uso de MSE en un modelo entrenado con pytorch. De ningún modo este modelo funciona, ni en este ni en ningún otro data set de regresión, pero se podria acomodar para que lo haga, empezando por agregar un DataLoader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "controlled-picking",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "maritime-vacation",
   "metadata": {},
   "outputs": [],
   "source": [
    "LR = 0.01 \n",
    "EPOCHS = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "weekly-investigation",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>longitude</th>\n",
       "      <th>latitude</th>\n",
       "      <th>housing_median_age</th>\n",
       "      <th>total_rooms</th>\n",
       "      <th>total_bedrooms</th>\n",
       "      <th>population</th>\n",
       "      <th>households</th>\n",
       "      <th>median_income</th>\n",
       "      <th>median_house_value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-114.31</td>\n",
       "      <td>34.19</td>\n",
       "      <td>15.0</td>\n",
       "      <td>5612.0</td>\n",
       "      <td>1283.0</td>\n",
       "      <td>1015.0</td>\n",
       "      <td>472.0</td>\n",
       "      <td>1.4936</td>\n",
       "      <td>66900.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-114.47</td>\n",
       "      <td>34.40</td>\n",
       "      <td>19.0</td>\n",
       "      <td>7650.0</td>\n",
       "      <td>1901.0</td>\n",
       "      <td>1129.0</td>\n",
       "      <td>463.0</td>\n",
       "      <td>1.8200</td>\n",
       "      <td>80100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-114.56</td>\n",
       "      <td>33.69</td>\n",
       "      <td>17.0</td>\n",
       "      <td>720.0</td>\n",
       "      <td>174.0</td>\n",
       "      <td>333.0</td>\n",
       "      <td>117.0</td>\n",
       "      <td>1.6509</td>\n",
       "      <td>85700.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-114.57</td>\n",
       "      <td>33.64</td>\n",
       "      <td>14.0</td>\n",
       "      <td>1501.0</td>\n",
       "      <td>337.0</td>\n",
       "      <td>515.0</td>\n",
       "      <td>226.0</td>\n",
       "      <td>3.1917</td>\n",
       "      <td>73400.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-114.57</td>\n",
       "      <td>33.57</td>\n",
       "      <td>20.0</td>\n",
       "      <td>1454.0</td>\n",
       "      <td>326.0</td>\n",
       "      <td>624.0</td>\n",
       "      <td>262.0</td>\n",
       "      <td>1.9250</td>\n",
       "      <td>65500.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   longitude  latitude  housing_median_age  total_rooms  total_bedrooms  \\\n",
       "0    -114.31     34.19                15.0       5612.0          1283.0   \n",
       "1    -114.47     34.40                19.0       7650.0          1901.0   \n",
       "2    -114.56     33.69                17.0        720.0           174.0   \n",
       "3    -114.57     33.64                14.0       1501.0           337.0   \n",
       "4    -114.57     33.57                20.0       1454.0           326.0   \n",
       "\n",
       "   population  households  median_income  median_house_value  \n",
       "0      1015.0       472.0         1.4936             66900.0  \n",
       "1      1129.0       463.0         1.8200             80100.0  \n",
       "2       333.0       117.0         1.6509             85700.0  \n",
       "3       515.0       226.0         3.1917             73400.0  \n",
       "4       624.0       262.0         1.9250             65500.0  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = pd.read_csv('./california_housing_train.csv')\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "accepted-thursday",
   "metadata": {},
   "outputs": [],
   "source": [
    "#clean empty values and inf\n",
    "train_df.replace([np.inf, -np.inf], np.nan)  # Replace inf values with NaNs\n",
    "train_df.dropna(inplace=True)  # Drop NaN values from the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "european-hampshire",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((17000, 8), (17000, 1))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#inputs, targets\n",
    "# Convert from Pandas dataframe to numpy arrays\n",
    "inputs = train_df.drop(['median_house_value'], axis=1).to_numpy()  # Drop TARGET_COLUMN since it is target, can not be used as input\n",
    "targets = train_df[['median_house_value']].to_numpy()\n",
    "inputs.shape, targets.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "level-mainland",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = torch.from_numpy(inputs)\n",
    "inputs = inputs.float()\n",
    "targets = torch.from_numpy(targets)\n",
    "targets = targets.float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "opposed-michigan",
   "metadata": {},
   "outputs": [],
   "source": [
    "class linearRegression(torch.nn.Module):\n",
    "    def __init__(self, in_features, out_features):\n",
    "        super(linearRegression, self).__init__()\n",
    "        self.linear = torch.nn.Linear(in_features, out_features)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.linear(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "environmental-characteristic",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = linearRegression(8, 1) #in_features = 8dim, out_features=1dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "norman-aberdeen",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = torch.nn.MSELoss() \n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=LR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "comprehensive-phrase",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input:  tensor([-1.1431e+02,  3.4190e+01,  1.5000e+01,  5.6120e+03,  1.2830e+03,\n",
      "         1.0150e+03,  4.7200e+02,  1.4936e+00])\n",
      "output:  tensor([-755.7535], grad_fn=<SelectBackward>)\n",
      "target:  tensor([66900.])\n",
      "epoch 0, loss 56654794752.0\n",
      "---------------------------------------------\n",
      "input:  tensor([-1.1431e+02,  3.4190e+01,  1.5000e+01,  5.6120e+03,  1.2830e+03,\n",
      "         1.0150e+03,  4.7200e+02,  1.4936e+00])\n",
      "output:  tensor([7.5477e+10], grad_fn=<SelectBackward>)\n",
      "target:  tensor([66900.])\n",
      "epoch 1, loss 2.8303811984632096e+21\n",
      "---------------------------------------------\n",
      "input:  tensor([-1.1431e+02,  3.4190e+01,  1.5000e+01,  5.6120e+03,  1.2830e+03,\n",
      "         1.0150e+03,  4.7200e+02,  1.4936e+00])\n",
      "output:  tensor([-2.3554e+16], grad_fn=<SelectBackward>)\n",
      "target:  tensor([66900.])\n",
      "epoch 2, loss 2.773957857786353e+32\n",
      "---------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(EPOCHS):\n",
    "    \n",
    "\n",
    "    # Clear gradient buffers because we don't want any gradient from previous epoch to carry forward, dont want to cummulate gradients\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # get output from the model, given the inputs\n",
    "    print('input: ', inputs[0])\n",
    "    outputs = model(inputs)\n",
    "    print('output: ', outputs[0])\n",
    "    print('target: ', targets[0])\n",
    "\n",
    "    # get loss for the predicted output\n",
    "    loss = criterion(outputs, targets)\n",
    "    #print(loss)\n",
    "        \n",
    "    # get gradients w.r.t to parameters\n",
    "    loss.backward()\n",
    "\n",
    "    # update parameters\n",
    "    optimizer.step()\n",
    "\n",
    "    print('epoch {}, loss {}'.format(epoch, loss.item()))\n",
    "    print('---------------------------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lonely-complex",
   "metadata": {},
   "source": [
    "## Cross-Entropy Loss \n",
    "\n",
    "Para problemas de clasificación vamos a utilizar Cross-Entropy Loss. Cross-entropy tambien es un concepto de proveniente de information theory que tampoco podría explicar, pero esta disponible [aquí](https://en.wikipedia.org/wiki/Cross_entropy).\n",
    "\n",
    "Dependiendo del problema de clasificación que tengamos entre mano, vamos a usar distintos tipos de cross-entropy, o más bien, vamos a aplicar distintas funciones de activación a nuestro vector de output.\n",
    "\n",
    "\n",
    "<img src=\"images.png\">\n",
    "\n",
    "\n",
    "Para problemas multi-clase (a qué intent pertenece una oración, por ejemplo), vamos a aplicar una función Softmax que convierta todos los valores de nuestro vector output en valores entre 0 y 1 de modo que sumen 1 entre sí. Así, cada elemento del output vector puede ser interpretado como la probabilidad de una clase.\n",
    "\n",
    "Para problemas de clasificación binaria o multi-label, la función de activación será la Sigmoid Function que también lleva todos los números a valores entre 0 y 1 pero sin que entre ellos sumen 1. Si pensamos en un problema con dos clases: el ejemplo recibirá un valor más cercano a 1 para la clase a la que pertenece y más cercano a 0 para la clase a la que no pertenece. Y lo mismo sucede para cada una de las clases a las que pertenece el ejemplo (multi-intent)\n",
    "\n",
    "\n",
    "-------------------------------------------------\n",
    "<sub>[Este post sobre cross-entropy](https://gombru.github.io/2018/05/23/cross_entropy_loss/)</sub>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faced-aquatic",
   "metadata": {},
   "source": [
    "### Categorical Cross-Entropy Loss \n",
    "\n",
    "Categorical-cross Entropy loss es usualmente aplicado en problemas de clasificación multi clase donde el output del modelo es interpretado como la probabilidad de que un dato pertenezca a una clase.\n",
    "\n",
    "En estos problemas vamos a tener outputs reunidos en un vector de salida de tamaño C (número de clases) y el ground truth va a ser un one-hot vector con 1 posición (clase) positiva y C-1 posiciones negativas. \n",
    "\n",
    "Como vimos, primero aplica la función Softmax y luego la cross-entropy loss.\n",
    "\n",
    "<img src=\"softmax.png\">\n",
    "\n",
    "En la función Softmax el resultado que para cada clase z_i depende de las demás clases (z_j)\n",
    "\n",
    "El ejemplo de [Categorical Cross Entropy](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html) que vamos a ver es nativo de pytorch y está descripto como:\n",
    "\n",
    "'This criterion combines LogSoftmax and NLLLoss in one single class.'\n",
    "\n",
    "La relación entre cross-entropy y NLLLoss (Negative Log-Likelihood) está también descripta en el artículo de Wikipedia sobre cross-entropy en information theory y tampoco puedo explicarlo ahora.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "psychological-president",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.3808, grad_fn=<NllLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "ce_loss = nn.CrossEntropyLoss()\n",
    "outputs = torch.randn(3, 5, requires_grad=True)\n",
    "targets = torch.tensor([1, 0, 3], dtype=torch.int64)\n",
    "loss = ce_loss(outputs, targets)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ordered-craft",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.4656, 0.6432, 0.3897, 0.2335, 0.8412], grad_fn=<SelectBackward>)\n",
      "tensor(1)\n"
     ]
    }
   ],
   "source": [
    "print(outputs[0])\n",
    "print(targets[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "furnished-cannon",
   "metadata": {},
   "outputs": [],
   "source": [
    "#To-do: cuenta manual\n",
    "# Cuenta propia para ver cómo funciona"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "disabled-harbor",
   "metadata": {},
   "source": [
    "### Clasificación Multi Clase con MNIST dataset (default available in colab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "adequate-sailing",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   0    1    2    3    4    5    6    7    8    9    ...  775  776  777  778  \\\n",
      "0    6    0    0    0    0    0    0    0    0    0  ...    0    0    0    0   \n",
      "1    5    0    0    0    0    0    0    0    0    0  ...    0    0    0    0   \n",
      "2    7    0    0    0    0    0    0    0    0    0  ...    0    0    0    0   \n",
      "3    9    0    0    0    0    0    0    0    0    0  ...    0    0    0    0   \n",
      "4    5    0    0    0    0    0    0    0    0    0  ...    0    0    0    0   \n",
      "\n",
      "   779  780  781  782  783  784  \n",
      "0    0    0    0    0    0    0  \n",
      "1    0    0    0    0    0    0  \n",
      "2    0    0    0    0    0    0  \n",
      "3    0    0    0    0    0    0  \n",
      "4    0    0    0    0    0    0  \n",
      "\n",
      "[5 rows x 785 columns]\n",
      "length columns:  785\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0, 300, 784)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = pd.read_csv('./mnist_train_small.csv', header=None, index_col=None)\n",
    "print(train_df.head())\n",
    "print('length columns: ', len(train_df.columns)) #28px x 28px = 784 + label = 1 --> 785 columns\n",
    "train_df.columns[0], train_df.columns[300], train_df.columns[784] #??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "beautiful-glucose",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((20000, 784), (20000,))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#inputs, labels\n",
    "# Convert from Pandas dataframe to numpy arrays\n",
    "inputs = train_df.drop([0], axis=1).to_numpy()  \n",
    "labels = train_df[0].to_numpy()\n",
    "inputs.shape, labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "parallel-albany",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6, 7, 8)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels[0], labels[18], labels[1880]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "encouraging-carry",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = torch.from_numpy(inputs)\n",
    "inputs = inputs.float()\n",
    "labels = torch.from_numpy(labels)\n",
    "labels = labels.type(torch.LongTensor) #la función cross-entropy espera un valor representando la categoria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "twelve-respect",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classification(torch.nn.Module):\n",
    "    def __init__(self, in_features, out_features):\n",
    "        super(Classification, self).__init__()\n",
    "        self.linear = torch.nn.Linear(in_features, out_features)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.linear(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "extensive-import",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Classification(784,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "conventional-talent",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = torch.nn.CrossEntropyLoss() \n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=LR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "oriental-haiti",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels:  tensor(6)\n",
      "tensor(1720.8148, grad_fn=<NllLossBackward>)\n",
      "epoch 0, loss 1720.8148193359375\n",
      "---------------------------------------------\n",
      "labels:  tensor(6)\n",
      "tensor(1122.7852, grad_fn=<NllLossBackward>)\n",
      "epoch 1, loss 1122.78515625\n",
      "---------------------------------------------\n",
      "labels:  tensor(6)\n",
      "tensor(886.0213, grad_fn=<NllLossBackward>)\n",
      "epoch 2, loss 886.0213012695312\n",
      "---------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(EPOCHS):\n",
    "    \n",
    "\n",
    "    # Clear gradient buffers because we don't want any gradient from previous epoch to carry forward, dont want to cummulate gradients\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # get output from the model, given the inputs\n",
    "    #print('input: ', inputs[0])\n",
    "    outputs = model(inputs)\n",
    "    print('labels: ', labels[0])\n",
    "    #print('output: ', loss_outputs)\n",
    "    # get loss for the predicted output\n",
    "    loss = criterion(outputs, labels)\n",
    "        \n",
    "    # get gradients w.r.t to parameters\n",
    "    loss.backward()\n",
    "\n",
    "    # update parameters\n",
    "    optimizer.step()\n",
    "\n",
    "    print('epoch {}, loss {}'.format(epoch, loss.item()))\n",
    "    print('---------------------------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "gross-somerset",
   "metadata": {},
   "source": [
    "### Binary Cross-Entropy Loss (binary classification, and multilabel)\n",
    "\n",
    "En los problemas de multi-labelling, cada muestra puede pertenecer a más de una clase. Es decir que el vector target ya no va a ser un one-hot vector, si no un vector de 0s y 1s con dimension C. Estos problemas se reformulan como C clasificaciones binarias independientes entre sí. Por esto, la BCELoss es tambien la función que utilizamos para clasificaciones binarias (como sentiment analysis)\n",
    "\n",
    "En este caso aplicamos primero la función de activación Sigmoid. Esta función se aplica independientemente a cada clase. También se la llama función logística. \n",
    "\n",
    "\n",
    "<img src=\"sigmoid.png\">\n",
    "\n",
    "Pytorch tiene dos implementaciones: [BCELoss](https://pytorch.org/docs/stable/generated/torch.nn.BCELoss.html) y [BCEWITHLOGITSLOSS](https://pytorch.org/docs/stable/generated/torch.nn.BCEWithLogitsLoss.html)\n",
    "\n",
    "La primera solo aplica la función loss mientras que la segunda combina la capa de función Sigmoid con la capa de loss. Notar cómo en el siguiente ejemplo se aplica la BCELoss y por eso, antes se calcula la sigmoidal de los outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "prospective-dancing",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.8484],\n",
      "        [0.5441],\n",
      "        [0.2829],\n",
      "        [0.5932]], grad_fn=<SigmoidBackward>)\n",
      "tensor(0.7780, grad_fn=<BinaryCrossEntropyBackward>)\n"
     ]
    }
   ],
   "source": [
    "bce_loss = nn.BCELoss()\n",
    "sigmoid = nn.Sigmoid()\n",
    "probabilities = sigmoid(torch.randn(4, 1, requires_grad=True))\n",
    "targets = torch.tensor([1, 0, 1, 0],  dtype=torch.float32).view(4, 1)\n",
    "loss = bce_loss(probabilities, targets)\n",
    "print(probabilities)\n",
    "print(loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "musical-malaysia",
   "metadata": {},
   "outputs": [],
   "source": [
    "#To-do: cuenta manual\n",
    "# Cuenta propia para ver cómo funciona"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "unexpected-final",
   "metadata": {},
   "outputs": [],
   "source": [
    "#To-do: ejemplo canónico de problema de claificación binaria."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "arctic-circuit",
   "metadata": {},
   "source": [
    "### Negative Log Likelihood for CRF\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "theoretical-chicken",
   "metadata": {},
   "source": [
    "### Combining Loss Functions for Multi Task Learning problems\n",
    "\n",
    "\n",
    "<img src=\"Diet.png\">"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
